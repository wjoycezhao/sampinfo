---
title: "Hierarchical Sampling Model"
output:  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

https://mc-stan.org/docs/2_23/functions-reference/index.html
https://mc-stan.org/docs/2_23/reference-manual/index.html

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=FALSE, message=FALSE}
library(sampinfo)
library(bayesplot)
library(rstan)
library(knitr)
```

Prepare the data:
--------

To begin with, we have a dataset with many participants, each finishing a same set of questions. Within each question, there are several time points, where the participant samples from a set of C options.  
In this toy dataset, total participant number $S=10$. The participant ID is listed in *sID*. 

```{r}
subset(sampinfo::raw_data, qID==1)
```

Total question number $Q = 5$. The question ID for each sampling time point is stored in *qID*.
```{r}
subset(sampinfo::raw_data, sID==1)
```

The time points within a question are saved in *tNo*. For the model fitting code with flexible decay to work, we need to make sure that sampling decisions for a same participant and same question are stored together in the formmated data, and that the *tNo*s start from 1 and are consecuuttive.  
There are in total 7 options, and choices are indicated using integers from 1 to 7. This variable is saved in *cID*,

Now we add feature values of the options to the dataset. In this example, we have three features: sameC, sameA and dist. 

* sameC: whether the option (i.e., thought cluster) is the same as the one sampled in the previous time point  
* sameA: whether they are decision congruent  
* dist: the semantic distance between the option and the previously sampled option  

```{r}
head(sampinfo::format_data,6)
```


Understand the model:
--------
### Basics:

Our goal is to predict the probabilities of sampling different options at each time point, using known features of the options. 

To start simple, let's use a single feature to predict sampling probabilities. For a trial with C options, we can then write the feature values into a length-$C$ column vector \[\boldsymbol{X_{C\times1}} = [x_1, x_2, ..., x_C]^T\]
Now we assume the effect can be captured by a linear multiplier $\beta$, and that the predicted choice $Y$ follows a categorical distribution \[Y \sim Cat(Softmax(\boldsymbol{X_{C\times1}}\beta))\]

In a case with $K$ features, the feature values can be stored in a $C\times K$ matrix, $\boldsymbol{X}$. 

In our formatted data, feature values for different options at time $t$ of participant $s$ in question $q$ is stored in a row. For example 
```{r}
subset(sampinfo::format_data,sID==1&qID==2&tNo==3)
```
For this row, the associated ferature matrix $\boldsymbol{X}$ is
```{r}
sampinfo::getFeatureMatrices(beta = c('sameC', 'sameA', 'dist'),
                             subset(sampinfo::format_data,sID==1&qID==2&tNo==3),
                             deltaM_value = 0, option_num = 7, print_names = T)[[1]]

```

Their effects can be captured by a length-$K$ vector $\boldsymbol{\beta}$. For example, in our example, $\boldsymbol{\beta} = [\beta_{sameC},\beta_{sameA},\beta_{dist}]^T$.

The predicted sampling probability \[Y \sim Cat(Softmax(\boldsymbol{X_{C\times K}}\boldsymbol{\beta_{K\times1}})\] 

Hereafter, to denote that the feature matrix $X$ varies among participants, questions and time points, we use superscripts to indicate to which participant, question and time point the matrix applies. For participant $s$, question $q$ and time point $t$:
\[Y^{s,q,t} \sim Cat(Softmax(\boldsymbol{X^{s,q,t}_{C\times K}}\boldsymbol{\beta_{K\times1}})\] 

Now we have a the model for predicting the sampling probability at each time point, using a single $\boldsymbol{\beta}$ for all the participants anad questions. Very simple.


### Hiearchical structure:

Now how about heterogeneity across participants and questions? We need a *hiearchical model* to address that. In this part we specify how to model the feature effects for participant $s$ in question $q$, i.e., $\boldsymbol{\beta^{s,q}}$, . 

To start, we assume there is a grand mean for each of the K features' effects. We denote the mean effects as length-$K$ column vector $\boldsymbol{\mu}$. 

We then assume that the $S$ participants deviate from the grand mean on each of the $K$ features. Here the deviations can be written as $\boldsymbol{\eta_{S}}$, a matrix of size $K \times S$. Column $s$ of this matrix, $\boldsymbol{\eta_{S}}[*,s]$ represents deviations of participant $s$ on the $K$ features. Similarly, the deviations of the $Q$ questions on the features can be written as $\boldsymbol{\eta_{Q}}$, a $K\times Q$ matrix. Column $q$ of this matrix, $\boldsymbol{\eta_{Q}}[*,q]$ represents deviations of question $q$ on the $K$ features.

Therefore, for participant $s$ and question $q$, the feature effects \[\boldsymbol{\beta^{s,q}}=\boldsymbol{\mu}+\boldsymbol{\eta_{S}}[*,s]+\boldsymbol{\eta_{Q}}[*,q]\]
the sampling choice 
\[Y^{s,q,t} \sim Cat(Softmax(\boldsymbol{X^{s,q,t}}\boldsymbol{\beta^{s,q}})\]

If we extract a row of of the deviation matrices, its elements reprersent deviations of different participants (or questions) on a single feature. We assume that for a single feature, the deviations of the participants (or questions) follows a centered Normal distribution. Under this assumption, we need scale parameters to quantify these deviations. For participant and question level deviations, the scale parameters are $\boldsymbol{\sigma_{S}}$ and $\boldsymbol{\sigma_{Q}}$ respectively. $\boldsymbol{\sigma_{S}}$ and $\boldsymbol{\sigma_{Q}}$ are both length-$K$ column vectors. Therefore for feature $k$, we have:
\[\boldsymbol{\eta_{S}}[k,*] \sim Normal(0,\boldsymbol{\sigma_{S}}[k])\\
\boldsymbol{\eta_{Q}}[k,*] \sim Normal(0,\boldsymbol{\sigma_{Q}}[k])\]
Or equivelently,
\[\boldsymbol{\theta_{S}}[k,*] \sim Normal(0,1),\ \boldsymbol{\eta_{S}}[k,*] = \boldsymbol{\sigma_{S}}[k] \boldsymbol{\theta_{S}}[k,*]\\
\boldsymbol{\theta_{Q}}[k,*] \sim Normal(0,1),\ \boldsymbol{\eta_{Q}}[k,*] = \boldsymbol{\sigma_{Q}}[k] \boldsymbol{\theta_{Q}}[k,*]\\
\]
To summarize, the parameters we need to fit include: 

- $\boldsymbol{\mu}$: a length-$K$ column vector for grand mean effects
- $\boldsymbol{\sigma_{S}}$: a length-$K$ column vector. SD for patcipant-level deviations
- $\boldsymbol{\sigma_{Q}}$: a length-$K$ column vector. SD for question-level deviations
- $\boldsymbol{\eta_{S}}$: a $K\times S$ matrix. All partcipant-level deviations.
- $\boldsymbol{\eta_{Q}}$: a $K\times Q$ matrix. All question-level deviations. 

Note that if we use the second parameterizatiton for $\boldsymbol{\eta_{S}}$ and $\boldsymbol{\eta_{Q}}$, then we are fitting $\boldsymbol{\theta_{S}}$ and $\boldsymbol{\theta_{Q}}$ in stead of $\boldsymbol{\eta_{S}}$ and $\boldsymbol{\eta_{Q}}$. For the differences between these two approaches, please refer to the discussion of *Nealâ€™s Funnel* in the last section of this document.

### Other specifications:
 
Beyond these we can also implement memory of previous time points, so that not only the feature values of the current time points influence sampling behaviors. In other words, we assume that at time point 1 of each question, the accumulative feature value matrix: \[\boldsymbol{A^{s,q,1}}=\boldsymbol{X^{s,q,1}}\]
And for $t > 1$,
\[\boldsymbol{A^{s,q,t}}=\delta\boldsymbol{A^{s,q,t-1}} + \boldsymbol{X^{s,q,t}}\]
The predicted sampling probability \[Y^{s,q,t} \sim Cat(Softmax(\boldsymbol{A^{s,q,t}}\boldsymbol{\beta^{s,q}})\] 

When $\delta=1$, participants have full memory. For participant 1 and question 2, there are three time points and the feature value matrices are
```{r}
sampinfo::getFeatureMatrices(beta = c('sameC', 'sameA', 'dist'),
                             subset(sampinfo::format_data,sID==1&qID==2),
                             deltaM_value = 0, option_num = 7, print_names = T)

```
The accumulative feature value matrix should be:
```{r}
sampinfo::getFeatureMatrices(beta = c('sameC', 'sameA', 'dist'),
                             subset(sampinfo::format_data,sID==1&qID==2),
                             deltaM_value = 1, option_num = 7, print_names = T)

```

Finally, for this dataset, we assume different base rates for different questions (no participant level difference on this matter). 


Now let's take a break from maths...

And have a look at the code.

### Stan codes:

```{r, comment = ">", highlight=FALSE}
sampinfo::getStanCode(0)
```

Next let's add a flexible decay parameter.
```{r, comment = ">", highlight=FALSE}
sampinfo::getStanCode(9)
```
Try fitting the model to the data:
--------

We will use a vector of feature names to specify which features to include in the model. For example:
```{r}
beta = c('sameC', 'sameA', 'dist')
```

If we need a baseline model without any features, set
```{r}
beta = c()
```


Next we need to decide whether we want decay in our model.  
If we assume there is no memory at all (i.e., Markov property), then we set
```{r}
deltaM_value = 0
```


On the other extreme, if no information is ever forgotten within a question (i.e., no decay). We set
```{r}
deltaM_value = 1
```


Finally we can estimate $\delta$ as a free parameter. Suppose we need a free parameter for $\delta$, then we can specify
```{r}
deltaM_value = 9
```
With the data, and the model specifications, we can now fit the model.   

```{r}
beta = c('sameA', 'dist')
stan_data_fit = sampinfo::getStanFit(beta = beta, deltaM_value = 9, 
                           option_num = 7, format_data = format_data,
                           save_model_file = NULL, init_values="random",
                           iter_num = 200,chain_num = 4,warmup_num = 100, core_num=4,
                           adapt_delta=0.9, stepsize = 0.1, max_treedepth = 10,
                           refresh=1000, save_warmup = TRUE)
```

To make sure that the samples from the posterior distributions are not biased, we need to make sure the chains converge. Only then are we sampling the stationary distribution of the Marco chain. Because many models take minutes/hours/days/... weeks to run, we need to choose a warm-up number carefully. Try some warmup number, test convergence, and repeat until things look good. 

Some methods to check model convergence.

- Check model convergence using traceplots.

```{r, dpi=300,   fig.height = 6, fig.width = 8, fig.align = "center", out.width = "85%"}
posterior_w = rstan::extract(stan_data_fit$stan_fit, inc_warmup = TRUE, permuted = FALSE)
dim(posterior_w)
lapply(dimnames(posterior_w), head)
bayesplot::color_scheme_set("viridis")
pars0 = c("beta_mu[1]","beta_s_sd[1]","beta_q_sd[1]","beta_s_raw[1,10]","deltaM","alpha[3,1]")
bayesplot::mcmc_trace(posterior_w, pars = pars0, 
                      n_warmup=100, size = 0.5,  
                      facet_args = list(nrow = 3))
```

- Another useful plot, which shows us if the ranks of the samples are mixed well among chains.

```{r, dpi=300,   fig.height = 6, fig.width = 8, fig.align = "center", out.width = "85%"}
bayesplot::mcmc_rank_overlay(posterior_w, pars = pars0)
```

- Also check gelman-rubin rhat.
```{r, warning=FALSE}
rhat = sampinfo::getRhat(stan_data_fit$stan_fit)
print(rhat[rhat>1.05&!is.na(rhat)])
```

Run the model:
--------

Now take the (conservative) warm-up number, and choose the total iteration number so that the sample size will be enough for the statistics of interests.
```{r, warning=FALSE}
stan_data_fit = sampinfo::getStanFit(beta= c('sameA', 'dist'), deltaM_value = 9, 
                           option_num = 7, format_data = format_data,
                           save_model_file = NULL, init_values="random",
                           iter_num = 1200,chain_num = 4,warmup_num = 300, core_num=4,
                           adapt_delta=0.9, stepsize = 0.1, max_treedepth = 10,
                           refresh=1000, save_warmup = TRUE)
```


Check the model diagnostics and goodness of fit:
--------

ESS is effective sample size, 1000 looks good in my opinion.

```{r}
model_diag = sampinfo::getModelDiag(stan_data_fit = stan_data_fit)
print(model_diag)
```

If you specify csv_name in getModelDiag, then the output will be saved in a .csv file. Check help(getModelDiag).

DIC and WAIC can be used for model comparisons. Lisheng also mentioned Bayes factors, LOO-CV. 

Good reference:    
German, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and computing, 24(6), 997-1016. [Link](https://arxiv.org/pdf/1307.5928.pdf)

Summarize and plot parameters
--------
```{r}
para = sampinfo::getParaSummary(stan_data_fit = stan_data_fit)
print(round(para[1:3,],1))
```
If you specify csv_name in getParaSummary, then the output will be saved in a .csv file. Check help(getParaSummary).

Here are the question level betas (grand mean plus question-level deviance)
```{r}
print(round(para[stringr::str_detect(rownames(para),'qmean'),c(1:3,8,13:14)],1))
```


and the participant level betas (grand mean plus participant-level deviance)

```{r}
print(round(para[stringr::str_detect(rownames(para),'smean'),c(1:3,8,13:14)],1))
```

Also option base rates in different questions.
```{r}
print(round(para[stringr::str_detect(rownames(para),'alpha'),c(1:3,8,13:14)],1)[1:20,])
```


There are many ways to visualize the samples. For example, check the bayesplot package [Link](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)

```{r, dpi=300,   fig.height = 3.6, fig.width = 3.6, fig.align = "center", out.width = "50%"}
posterior = rstan::extract(stan_data_fit$stan_fit, inc_warmup = FALSE, permuted = FALSE)
dim(posterior)
## run changeBetaNames to make the parameter names understandable (so that we dont need to deal with matrix index)
dimnames(posterior)[[3]] = sampinfo::changeBetaNames(dimnames(posterior)[[3]],
                            beta = beta,
                            Q = max(format_data$qID), 
                            S = max(format_data$sID))

bayesplot::mcmc_intervals(posterior, regex_pars = "^beta_dist_qmean")
```


Additional diagnostics in Rstan
--------

There are other diagnostics to pay attention to if the model is fit using Stan (HMC and NUTS)
```{r}
model_diag = getModelDiag(stan_data_fit = stan_data_fit)
print(model_diag[,c('divergent', 'max_tree')])
```


For example we need to make sure there is no divergent transitions, which is not uncommon for hierarchical models. When it happens we often need to: 

- debug the rstan code
- try recovering the parameters 
- reparameterize the model (for example, see Neal's tunnel) 
- increase alpha_delta, increase tree_maxdepth and decrease stepsize. We can explain this if time allows. Otherwise please refer to the part on HMC and NUTS in the next/last section.

If the maximum tree depth is hit in NUTS, we need to increase max_treedepth.  

RStan will send warnings regarding these issues (remember to read it if the models are fit on GPC).

Other topics:
--------
### Nealâ€™s Funnel

Note that there are two ways to parameterize Normal distributions (in hierarchical models): 

* Centered parameterization:    
\[\beta \sim norml(\mu_{\beta}, \sigma_{\beta})\]
* Non-centered parameterization:   
\[\beta_{raw} \sim norml(0, 1) \]
\[\beta = \mu_{\beta} + \sigma_{\beta} \beta_{raw} \]


If the individual level data are not very informative (and thus the individual parameters, $\beta$, are influenced by the group level priors $\mu_{\beta}$ and $\sigma_{\beta}$ a lot), it is much better to use the non-centered approach.
Detailed explanations can be found here:  

* RStan manual 22.7 [Link](https://mc-stan.org/docs/2_23/stan-users-guide/reparameterization-section.html)
* Betancourt, M., & Girolami, M. (2015). Hamiltonian Monte Carlo for hierarchical models.   Current trends in Bayesian methodology with applications, 79(30), 2-4. [Link](https://arxiv.org/pdf/1312.0906.pdf)
  
Intuitively, when $\sigma_{\beta}$ is small, $\beta$ tends to be very close to $\mu_{\beta}$. When $\sigma_{\beta}$ is large, $\beta$ varies a lot. That forms a tunnel shape. The stepsize needed for exploring the 'neck' is much much smaller than that required for the 'body'. Hence there's no single stepsize that stan can use to sample efficiently.  
If we use the non-centered parameterization, then conditional on the data, the actively sampled parameters becomes independent from each other. 

### HMC and Nuts

- Hamiltonian Monte Carlo explained. [Link](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)
- Betancourt, M. (2017). A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434. [Link](https://arxiv.org/pdf/1701.02434.pdf)
- Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. [Link](http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf)



