---
title: "Hierarchical model for sampling"
output: rmarkdown::html_vignette
# output:
#   md_document:
#     variant: markdown_github
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=FALSE, message=FALSE}
library(sampinfo)
library(bayesplot)
library(rstan)
library(knitr)
```

Prepare your data:
--------

To begin with, we have a dataset with many participants, each finishing a same set of questions. Within each question, there are several time points, where participants sample from a set of C options.  
In this toy dataset, total question number $Q = 5$. The question ID for each sampling time point is stored in qID.
```{r}
subset(sampinfo::raw_data,sID==1)
```

Total participant number $S=10$. The participant ID is listed in sID. 

```{r}
subset(sampinfo::raw_data,qID==1)
```

In each question, there are many time points, where participant sample from different options. Here time points are saved in tNo. There are in total 7 options (cID from 1 to 7).

Understand the model:
--------

Our goal is to predict the probabilities of sampling different options at each time point, using several known features of the options. 

In this example, we have three features: sameC, sameA and dist. 

* sameC: whether the option (i.e., thought cluster) is the same as the one sampled in the previous time point  
* sameA: whether they are decision congruent  
* dist: the semantic distance between the option and the previously sampled option  

```{r}
head(sampinfo::format_data,3)
```

Suppose that we use a single feature, then for a trial with C options, we write the feature values into a vector \[\boldsymbol{X_{C\times1}} = [X_1, ..., X_C]^T\]
Now assume the effect can be captured by a linear multiplier $\beta$, and that the predicted choice $Y$ follows a categorical distribution \[Y \sim Cat(softmax(\boldsymbol{X}_{C\times1}\beta))\]
In a case with $K$ features, the feature values become a matrix, $\boldsymbol{X_{C\times K}}$, and the effects can be captured by a vector $\boldsymbol{\beta_{K\times1}}$. 
The predicted sampling probability \[Y \sim Cat(softmax(\boldsymbol{X_{C\times K}}\boldsymbol{\beta_{K\times1}})\] 

This is the model for predicting the sampling probability in one time point, of one question, completed by one participant. 

Now how about heterogeneity across participants and questions? We need a *hiearchical model* to address that. 

We first assume there is a grand mean for each of the feature effects. We denote the mean effects as $\boldsymbol{\mu^{\beta}_{K\times1}}$. 

The $S$ participants deviate from the grand mean on each of the $K$ features. Here the deviation can be written as $\boldsymbol{\eta^{\beta,S}_{K\times S}}$. 

Similarly, the deviations of the $Q$ questions on the features can be written as $\boldsymbol{\eta^{\beta,Q}_{K\times Q}}$. 

We assume that each row of these deviation matrices follows a centered normal distribution. Under this assumption, we need scale parameters to quantify these deviations. For participant and question level deviations, the scale parameters are $\boldsymbol{\sigma^{\beta,S}_{K\times 1}}$ and $\boldsymbol{\sigma^{\beta,Q}_{K\times 1}}$ respectively.


To summarize, the parameters we need to fit include: $\boldsymbol{\mu^{\beta}_{K\times1}}$, $\boldsymbol{\sigma^{\beta,S}_{K\times 1}}$, $\boldsymbol{\sigma^{\beta,Q}_{K\times 1}}$, $\boldsymbol{\eta^{\beta,S}_{K\times S}}$, and $\boldsymbol{\eta^{\beta,Q}_{K\times Q}}$. For participant $s$ and question $q$, the predicted sampling choice 
\[Y \sim Cat(softmax(\boldsymbol{X_{C\times K}}(\boldsymbol{\mu^{\beta}_{K\times1}} + \boldsymbol{\eta^{\beta,S}_{*,s}} + \boldsymbol{\eta^{\beta,Q}_{*,q}}))\]
 
Beyond these we can also have decay, and different base rates for different questions.

Now let's take a break from maths...

And take a look at the code.

```{r, comment = ">", highlight=FALSE}
sampinfo::getStanCode(0)
```

Next let's add a flexible decay parameter.
```{r, comment = ">", highlight=FALSE}
sampinfo::getStanCode(9)
```
Try fitting your model to the data:
--------

We will use a vector of feature names to specify the which features to be included in the model. For example:
```{r}
beta = c('sameC', 'sameA', 'dist')
```

If you need a baseline model without any features, set
```{r}
beta = c()
```


Next we need to decide whether we want decay in our model.  
If we assume there is no memory at all (i.e., Markov property), then we set
```{r}
deltaM_value = 0
```


On the other extreme, if no information is ever forgotten within a question (i.e., no decay). We set
```{r}
deltaM_value = 1
```


Finally we can estimate delta as a free parameter. Suppose we need a free parameter for delta, then we can specify
```{r}
deltaM_value = 9
```
With the data, and the model specifications, we can now run the following to fit the model.   
To make sure that the samples from the posterior distributions are not biased, we need to make sure the chains converge. Only then are we sampling the stationary distribution of the Marco chain. Because many models take minutes/hours/days/... weeks to run, we need to choose a warm-up number carefully. Try some warmup number, test convergence, and repeat until things look good. 

```{r, warning=FALSE}
beta = c('sameA', 'dist')
stan_data_fit = sampinfo::getStanFit(beta= c('sameA', 'dist'), deltaM_value = 9, 
                           option_num = 7, format_data = format_data,
                           save_model_file = NULL, init_values="random",
                           iter_num = 1200,chain_num = 4,warmup_num = 300, core_num=4,
                           adapt_delta=0.9, stepsize = 0.1, max_treedepth = 10,
                           refresh=1000, save_warmup = TRUE)
```


Check the model diagnostics and goodness of fit:
--------

ESS is effective sample size, 1000 looks good in my opinion.

```{r}
model_diag = sampinfo::getModelDiag(stan_data_fit = stan_data_fit)
print(model_diag)
```


Summarize and plot parameters
--------
```{r}
para = sampinfo::getParaSummary(stan_data_fit = stan_data_fit)
print(round(para[1:3,],1))
```


Here are the question level betas (grand mean plus question-level deviance)
```{r}
print(round(para[stringr::str_detect(rownames(para),'qmean'),c(1:3,8,13:14)],1))
```


and the participant level betas (grand mean plus participant-level deviance)

```{r}
print(round(para[stringr::str_detect(rownames(para),'smean'),c(1:3,8,13:14)],1))
```

Also option base rates in differnet questions.
```{r}
print(round(para[stringr::str_detect(rownames(para),'alpha'),c(1:3,8,13:14)],1)[1:20,])
```


There are many ways to visualize the samples. For example, check the bayesplot package [Link](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)

```{r, dpi=300,   fig.height = 3.6, fig.width = 3.6, fig.align = "center", out.width = "50%"}
posterior = rstan::extract(stan_data_fit$stan_fit, inc_warmup = TRUE, permuted = FALSE)
dim(posterior)
head(dimnames(posterior)[[3]])
## run changeBetaNames to make the parameter names understandable (so that we dont need to deal with matrix index)
dimnames(posterior)[[3]] = sampinfo::changeBetaNames(dimnames(posterior)[[3]],
                            beta = beta,
                            Q = max(format_data$qID), 
                            S = max(format_data$sID))

bayesplot::mcmc_intervals(posterior, regex_pars = "^beta_dist_qmean")
```




